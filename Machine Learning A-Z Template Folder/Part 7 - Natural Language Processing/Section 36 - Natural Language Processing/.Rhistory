install.packages(c("jsonlite", "curl"))
require("jsonlite")
f1 <- fromJSON('http://ergast.com/api/f1/2008/results.json')
install.packages("LearnBayes")
install.packages("BaM")
install.packages("AtelieR")
q()
# Welcome to the course
x<-rnorm(100())
x<-rnorm(100)
head(x)
tail(x)
x=rnorm(100)
x<-rnorm(100)
mean(x)
sd(x)
mean(exp(rnorm(1000)))
exp(0-1/2)
exp(0+1/2)
mean(exp(rnorm(1000)))
x<-rnorm(1000)
y<-exp(x)
mean(y)
2+2
7*17
sqrt(9)
log(7)
log10(7)
3^3
sin(pi/2)
pi
options(digits=22)
pi
1/0
2*Inf
-1/0
0/0
c(1,2,3,NA,5)
mean(c(1,2,3,NA,5))
x <-5
x
x=5
x
x=9
x
x <- 5
x*x
y <- x+5
ls()
rm(x)
ls()
rm(y)
x <-2
x^2+3*x+1
y <-2
options(digits=2)
rm(x)
(-3+sqrt(3^2-4*1*1))/2*1
rm(y)
A<-1
B<-3
C<-1
my.vector<-c((-B+sqrt(B^2-$*A*C))/(2*A),(-B-sqrt(B^2-4*A*C))/(2*A))
my.vector<-c((-B+sqrt(B^2-4*A*C))/(2*A),(-B-sqrt(B^2-4*A*C))/(2*A))
my.vector
options(digits=1)
my.vector
?rbind
?c
my.vector<-c((-B+sqrt(B^2-4*A*C))/(2*A),(-B-sqrt(B^2-4*A*C))/(2*A))
gplot(my.vector)
plot(my.vector)
rm(all)
rm(A)
rm(B)
rm(C,my.vector)
x <- rnorm(100, mean=.5, sd=.3)
mean(x)
sd(x)
?histogram
?plot
plot(h,x)
plot(x,h)
plot(x, type = "h")
?plot
set.seed(1234)
mean(x)
sd(x)
??histogram
hist(x)
hist(x,axis=FALSE)
hist(x,axis = FALSE)
warnings()
hist(x,axes = FALSE)
axis(4)
axis(1)
set.seed(1234)
x<-rnorm(100,mean=.5,sd=.3)
mean(x)
mean(mean(x))
options(digits=10)
mean(x)
sd(x)
rm(x)
set.seed(1)
x <- rnorm(100)
head(x)
?tail
?"tail"
help("tail")
internal(tail())
help("tail")
x <-5:6
x
x <- 5+6
y <- x+3
z <- y-10
z
rm(x,y.z)
rm(x,y,z)
rm y,z
rm(y,z)
A <- (1,3,5,7,9)
A <- (1 3 5 7 9)
A <- seq(1,10,2)
B <- mean(A)
X <-seq(2,10,2)
Z <- A + X
rm.all
rm(A,B,X,Z)
?paste
paste("R session", 1)
data.fram(x=1:3,7=4:6)+2
data.fram(x=1:3,y=4:6)+2
data.frame(x=1:3,y=4:6)+2
data.frame(x=1:3,y=A:C)+2
data.frame(x=1:3,y="A","B","C")+2
data.frame(x=1:3,y=c("A","B","C")+2
data.frame(x=1:3,y=c("A","B","C"))+2
fib <- ((phi^n)-(-phi^n))/sqrt(5)
phi <- (sqrt(5)+1)/2
fib <- ((phi^n)-(-phi^-n))/sqrt(5)
fib(n) <- ((phi^n)-(-phi^-n))/sqrt(5)
n=0
fib(n) <- ((phi^n)-(-phi^-n))/sqrt(5)
fib <- ((phi^n)-(-phi^-n))/sqrt(5)
rm(fib)
rm(n)
rm(phi)
my.standard <- function(x){(x-mean(x))/sd(x)}
mystandard(5)
my.standard(5)
my.standard(1)
my.function <- function(x) {(3*sin(x/2))+x}
my.function(0)
my.function(-1)
my.function(pi)
source("/Users/david.noland/Learning/r/sinfunc.r")
y <- c(1, 2, 3.14, 4, 5)
y
z <- c(FALSE, TRUE, 2, 3, 4)
z
matrix(c(5,4,3,2,1,0)+2,nrow=2)<5
?sin
addsub<-function(x,y){ return(c(x+y,x-y)) }
addsubb(1,2)
addsub(1,2)
addsub()
addsub<-function(x=0,y){ return(c(x+y,x-y)) }
addsub()
addsub<-function(x=0,y=0){ return(c(x+y,x-y)) }
addsub()
addsub(2,3)
plot(sin, -pi, pi)
?cat
?density
?cat
?summary
my.display <- function(x) {}
?cat
?if
?
??if
asdfa
?if()
??if
asfd
help(if)
?ifelse
echo
?print
?hist
source("/Users/david.noland/Learning/r/mydisplay.r")
source("/Users/david.noland/Learning/r/mydisplay.r")
source("/Users/david.noland/Learning/r/mydisplay.r")
source("/Users/david.noland/Learning/r/mydisplay.r")
source("/Users/david.noland/Learning/r/mydisplay.r")
?"if"
source("/Users/david.noland/Learning/r/mydisplay.r")
set.seed(1234)
my.data<-rnorm(200)
my.display(my.data)
mydisplay(my.data)
my.display(my.data,display=TRUE,type="hist")
mydisplay(my.data,display=TRUE,type="hist")
mydisplay(my.data,display=TRUE,type="hist",prob=TRUE))
mydisplay(my.data,display=TRUE,type="hist",prob=TRUE)
mydisplay(my.data,display=TRUE)
plot(x)
plot(mydisplay)
set.seed(1786)
data.exercise.3.1<-exp(matrix(rnorm(2000),nrow=100))
index1.temp<-sample(1:100,10)
index2.temp<-sample(1:20,10)
for (in in 1:10) {}
source("/Users/david.noland/Learning/r/data.exercise.3.1.r")
my.data(data.exercise.3.1)
my.data <- data.exercise.3.1
for (i in my.data) {}
for (i in my.data) {say i}
my.data
my.data
my.data
max.print(150)
option(max.print=150)
options(max.print=150)
my.data
options(max.print=150000)
my.data
my.data
negative<-(min(my.data[i,])<0)
count.negatives<-count.negatives + negative
count.negatives <- count.negatives + negative
count.negatives <- 0
count.negatives <- count.negatives + negative
if (count.negatives>=3 & !negative){ cat("The mean of row",i,"is",mean(my.data[i,],"\n")}
if (count.negatives>=3 & !negative) {cat("The mean of row",i,"is",mean(my.data[i,],"\n")}
if (count.negatives>=3 & !negative) { cat("The mean of row",i,"is",mean(my.data[i,],"\n") }
my.data<-data.exercise.3.1
my.data
source("/Users/david.noland/Learning/r/data.exercise.3.1.r")
source("/Users/david.noland/Learning/r/data.exercise.3.1.r")
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age), FUN = function(x) mean(x, na.rm = TRUE),
dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary), FUN = function(x) mean(x, na.rm = TRUE),
dataset$Salary)
# Data Preprocessing
# Importing the dataset
dataset = read.csv('Data.csv')
# Taking care of missing data
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age), FUN = function(x) mean(x, na.rm = TRUE),
dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary), FUN = function(x) mean(x, na.rm = TRUE),
dataset$Salary)
# Data Preprocessing
# Importing the dataset
dataset = read.csv('Data.csv')
# Taking care of missing data
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
# Data Preprocessing
# Importing the dataset
dataset = read.csv('Data.csv')
# Taking care of missing data
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
library(readr)
Data <- read_csv("~/Learning/DataScience/Machine_Learning/Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Data.csv")
View(Data)
dataset = read.csv('Data.csv')
Data$Age = ifelse(is.na(Data$Age),
ave(Data$Age, FUN = function(x) mean(x, na.rm = TRUE)),
Data$Age)
Data$Salary = ifelse(is.na(Data$Salary),
ave(Data$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
Data$Salary)
View(Data)
setwd("~/Learning/DataScience/Machine_Learning/Machine Learning A-Z Template Folder/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing")
# Natural Language Processing
#Importing the Dataset
dataset = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
#Cleaning the texts
#install.packages('tm')
#install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
as.character(corpus[[481]])
as.character(corpus[[841]])
corpus = tm_map(corpus, stripWhitespace)
as.character(corpus[[841]])
corpus
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.99)
dtm
# Natural Language Processing
#Importing the Dataset
dataset = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
#Cleaning the texts
#install.packages('tm')
#install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
#Create the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.999)
dtm
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
View(dataset)
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
dataset$Liked = dataset_original$Liked
# Natural Language Processing
#Importing the Dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
#Cleaning the texts
#install.packages('tm')
#install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
#Create the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
#Importing the Dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
#Cleaning the texts
#install.packages('tm')
#install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
#Create the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
dataset$Liked = dataset_original$Liked
# Natural Language Processing
# Importing the dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
# Cleaning the texts
# install.packages('tm')
# install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Natural Language Processing
# Importing the dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
# Cleaning the texts
# install.packages('tm')
# install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Encoding the target feature as factor
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting classifier to the Training set
#install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Making the Confusion Matrix
cm = table(test_set[, 692], y_pred)
cm
(79+70)/200
